{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 1: Verify GPU availability for LLM inference\n"
      ],
      "metadata": {
        "id": "q1GdLZafcRmW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdwdiCzocNlj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 2: Install and update Hugging Face and acceleration libraries\n"
      ],
      "metadata": {
        "id": "ar_oVVJtgMH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate transformers\n",
        "!pip install -q torch sentencepiece tqdm pandas huggingface_hub"
      ],
      "metadata": {
        "id": "0wkhrvafgOWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 3: Add HuggingFace Token and Verify."
      ],
      "metadata": {
        "id": "ys0OdrAGglFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"huggingface_token_here\"\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to Hugging Face\")\n",
        "\n",
        "# HuggingFace GUI Token Input.\n",
        "\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iuSlAOCFgrSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 4: Define all the models.\n",
        "• We load three instruction-tuned LLMs to evaluate mathematical reasoning performance.\n",
        "\n",
        "• DeepSeek-Math-7B-Instruct serves as a high-capacity model for complex, multi-step math problems.\n",
        "\n",
        "• Qwen2.5-Math-7B-Instruct provides a comparable large model with a different training strategy for fair comparison.\n",
        "\n",
        "• Phi-3.5-mini is a lightweight Microsoft model used to analyze efficiency vs accuracy trade-offs.\n",
        "\n",
        "• All models are downloaded once and cached locally, ensuring faster execution and reproducible results."
      ],
      "metadata": {
        "id": "3TyJqxK5hiJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ONE-TIME SETUP: Download all models to cache without loading into RAM\n",
        "Run this once to download all models, then use the memory-efficient loader\n",
        "\"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "import gc\n",
        "\n",
        "CACHE_DIR = \"/content/gdrive/MyDrive/LLM_Math_Cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "MODELS = [\n",
        "    {\"name\": \"DeepSeek-Math-7B\", \"repo\": \"deepseek-ai/DeepSeek-Math-7B-Instruct\"},\n",
        "    {\"name\": \"Qwen2.5-Math-7B-Instruct\", \"repo\": \"Qwen/Qwen2.5-Math-7B-Instruct\"},\n",
        "    {\"name\": \"Phi-3.5-mini\", \"repo\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
        "]\n",
        "\n",
        "def is_model_cached(cache_path):\n",
        "    \"\"\"Check if model is already downloaded.\"\"\"\n",
        "    if not os.path.exists(cache_path):\n",
        "        return False\n",
        "    try:\n",
        "        files = os.listdir(cache_path)\n",
        "        has_config = \"config.json\" in files\n",
        "        has_weights = any(f.endswith(\".safetensors\") or f.endswith(\".bin\") for f in files)\n",
        "\n",
        "        snapshots_dir = os.path.join(cache_path, \"snapshots\")\n",
        "        if os.path.exists(snapshots_dir):\n",
        "            for snapshot in os.listdir(snapshots_dir):\n",
        "                snapshot_path = os.path.join(snapshots_dir, snapshot)\n",
        "                if os.path.isdir(snapshot_path):\n",
        "                    snapshot_files = os.listdir(snapshot_path)\n",
        "                    has_config = has_config or \"config.json\" in snapshot_files\n",
        "                    has_weights = has_weights or any(\n",
        "                        f.endswith(\".safetensors\") or f.endswith(\".bin\") for f in snapshot_files\n",
        "                    )\n",
        "        return has_config and has_weights\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DOWNLOADING ALL MODELS TO CACHE (One-time Setup)\")\n",
        "print(\"=\"*70)\n",
        "print(\"This will download models WITHOUT loading them into RAM\")\n",
        "print(\"After this completes, use the memory-efficient loader to test them\\n\")\n",
        "\n",
        "try:\n",
        "    HF_TOKEN\n",
        "except NameError:\n",
        "    print(\"ERROR: HF_TOKEN not found! Run the login cell first.\")\n",
        "    raise\n",
        "\n",
        "for i, m in enumerate(MODELS, 1):\n",
        "    repo = m[\"repo\"]\n",
        "    name = m[\"name\"]\n",
        "    cache_subdir = repo.replace(\"/\", \"--\")\n",
        "    cache_path = os.path.join(CACHE_DIR, cache_subdir)\n",
        "\n",
        "    print(f\"\\n[{i}/{len(MODELS)}] {name}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if is_model_cached(cache_path):\n",
        "        print(\"✓ Already cached, skipping download\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print(\"Downloading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            repo,\n",
        "            token=HF_TOKEN,\n",
        "            trust_remote_code=True,\n",
        "            cache_dir=CACHE_DIR,\n",
        "        )\n",
        "        del tokenizer\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Downloading model files (this may take several minutes)...\")\n",
        "        # Download model files without loading into RAM\n",
        "        from huggingface_hub import snapshot_download\n",
        "        snapshot_download(\n",
        "            repo_id=repo,\n",
        "            cache_dir=CACHE_DIR,\n",
        "            token=HF_TOKEN,\n",
        "            ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.md\"]  # Skip unnecessary files\n",
        "        )\n",
        "\n",
        "        print(f\"✓ {name} downloaded successfully\")\n",
        "\n",
        "        # Clear memory\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED: {str(e)[:200]}\")\n",
        "        if \"gated\" in str(e).lower() or \"access\" in str(e).lower():\n",
        "            print(f\"   → Visit https://huggingface.co/{repo} and accept the license\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DOWNLOAD COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext step: Use the memory-efficient loader to test models one at a time\")\n",
        "print(\"Example:\")\n",
        "print(\"  pipe = model_loaders['Deepseek-Math-7B']()\")\n",
        "print(\"  result = pipe('What is 2+2?')\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "gec5PMNHhjSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 5: Check the cached model and initialize the 4-bit quantization\n",
        "• Configures memory-efficient 4-bit quantization to load large language models with reduced GPU and CPU usage.\n",
        "\n",
        "• Checks whether the DeepSeek-Math-7B model is already cached locally and avoids re-downloading if present.\n",
        "\n",
        "• Implements lazy loading, ensuring the model is loaded only when required for inference.\n",
        "\n",
        "• Creates a deterministic text-generation pipeline, enabling reproducible mathematical reasoning experiments."
      ],
      "metadata": {
        "id": "VZfY_Mv_L7eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# 4-bit config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "CACHE_DIR = \"/content/gdrive/MyDrive/LLM_Math_Cache\"\n",
        "\n",
        "MODELS = [\n",
        "\n",
        "    {\"name\": \"Deepseek-Math-7B\", \"repo\": \"deepseek-ai/deepseek-math-7b-instruct\"},\n",
        "]\n",
        "\n",
        "def is_model_cached(repo):\n",
        "    \"\"\"Check if model is fully downloaded by checking HF cache structure.\"\"\"\n",
        "    # HuggingFace uses models--org--name format\n",
        "    cache_subdir = f\"models--{repo.replace('/', '--')}\"\n",
        "    cache_path = os.path.join(CACHE_DIR, cache_subdir)\n",
        "\n",
        "    if not os.path.exists(cache_path):\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Check for snapshots directory (HF cache structure)\n",
        "        snapshots_dir = os.path.join(cache_path, \"snapshots\")\n",
        "        if not os.path.exists(snapshots_dir):\n",
        "            return False\n",
        "\n",
        "        # Check if any snapshot exists and has model files\n",
        "        snapshots = os.listdir(snapshots_dir)\n",
        "        if not snapshots:\n",
        "            return False\n",
        "\n",
        "        # Check the latest snapshot\n",
        "        for snapshot in snapshots:\n",
        "            snapshot_path = os.path.join(snapshots_dir, snapshot)\n",
        "            if os.path.isdir(snapshot_path):\n",
        "                snapshot_files = os.listdir(snapshot_path)\n",
        "                has_config = \"config.json\" in snapshot_files\n",
        "                has_tokenizer = any(\"tokenizer\" in f for f in snapshot_files)\n",
        "                has_weights = any(\n",
        "                    f.endswith(\".safetensors\") or f.endswith(\".bin\")\n",
        "                    for f in snapshot_files\n",
        "                )\n",
        "\n",
        "                if has_config and has_tokenizer and has_weights:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning checking cache: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_single_model(repo, name, local_files_only=False):\n",
        "    \"\"\"Load a single model and return pipeline.\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            repo,\n",
        "            token=HF_TOKEN,\n",
        "            trust_remote_code=True,\n",
        "            cache_dir=CACHE_DIR,\n",
        "            local_files_only=local_files_only,\n",
        "            force_download=False,\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Special handling for Phi-3.5-mini\n",
        "        if \"Deepseek-Math-7B\" in name:\n",
        "            print(f\"  → Using eager attention for Phi-3.5-mini\")\n",
        "            model_kwargs = {\n",
        "                \"quantization_config\": quant_config,\n",
        "                \"device_map\": \"auto\",\n",
        "                \"torch_dtype\": torch.bfloat16,\n",
        "                \"trust_remote_code\": False,\n",
        "                \"attn_implementation\": \"eager\",\n",
        "                \"token\": HF_TOKEN,\n",
        "                \"cache_dir\": CACHE_DIR,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"low_cpu_mem_usage\": True\n",
        "            }\n",
        "        else:\n",
        "            model_kwargs = {\n",
        "                \"quantization_config\": quant_config,\n",
        "                \"device_map\": \"auto\",\n",
        "                \"torch_dtype\": torch.bfloat16,\n",
        "                \"trust_remote_code\": True,\n",
        "                \"token\": HF_TOKEN,\n",
        "                \"cache_dir\": CACHE_DIR,\n",
        "                \"local_files_only\": local_files_only,\n",
        "                \"low_cpu_mem_usage\": True\n",
        "            }\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(repo, **model_kwargs)\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=2048,\n",
        "            temperature=0.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"  → Error: {str(e)[:200]}\")\n",
        "        return None\n",
        "\n",
        "def get_model_loader(model_name):\n",
        "    \"\"\"\n",
        "    Factory function that returns a loader for a specific model.\n",
        "    This allows lazy loading - models are only loaded when you call the function.\n",
        "    \"\"\"\n",
        "    repo = None\n",
        "    for m in MODELS:\n",
        "        if m[\"name\"] == model_name:\n",
        "            repo = m[\"repo\"]\n",
        "            break\n",
        "\n",
        "    if repo is None:\n",
        "        raise ValueError(f\"Model {model_name} not found\")\n",
        "\n",
        "    # Check if cached\n",
        "    local_files_only = is_model_cached(repo)\n",
        "\n",
        "    def loader():\n",
        "        \"\"\"Load this specific model on demand.\"\"\"\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        pipe = load_single_model(repo, model_name, local_files_only)\n",
        "        if pipe:\n",
        "            print(f\"✓ {model_name} loaded successfully\")\n",
        "        return pipe\n",
        "\n",
        "    return loader\n",
        "\n",
        "# Verify token\n",
        "try:\n",
        "    HF_TOKEN\n",
        "    print(f\"✓ HF_TOKEN is set\\n\")\n",
        "except NameError:\n",
        "    print(\"⚠️  ERROR: HF_TOKEN not found! Run the login cell first.\")\n",
        "    raise\n",
        "\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MEMORY-EFFICIENT MODEL LOADER\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Cache: {CACHE_DIR}\\n\")\n",
        "\n",
        "# Check which models are cached\n",
        "print(\"Checking cache status...\\n\")\n",
        "model_loaders = {}\n",
        "cached_count = 0\n",
        "\n",
        "for m in MODELS:\n",
        "    repo = m[\"repo\"]\n",
        "    name = m[\"name\"]\n",
        "\n",
        "    is_cached = is_model_cached(repo)\n",
        "    status = \"✓ CACHED\" if is_cached else \"✗ NOT CACHED\"\n",
        "\n",
        "    # Show actual folder name for debugging\n",
        "    cache_folder = f\"models--{repo.replace('/', '--')}\"\n",
        "    cache_path = os.path.join(CACHE_DIR, cache_folder)\n",
        "    exists = \"EXISTS\" if os.path.exists(cache_path) else \"MISSING\"\n",
        "\n",
        "    print(f\"{status:15} {name:20} ({exists})\")\n",
        "\n",
        "    if is_cached:\n",
        "        cached_count += 1\n",
        "\n",
        "    # Create loader function for this model\n",
        "    model_loaders[name] = get_model_loader(name)\n",
        "\n",
        "print(f\"\\n{cached_count}/{len(MODELS)} models are cached\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n✓ model_loaders dictionary created!\")\n",
        "print(f\"✓ Ready to run inference on {cached_count} cached models\")\n",
        "print(\"\\nNext: Run the inference cell to test all models\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "ix_2D6loM6Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 6: Mount GDrive and Load JSON File."
      ],
      "metadata": {
        "id": "rGnyuNQeN9YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os, shutil\n",
        "\n",
        "# 1) Mount your Google Drive at /content/gdrive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 2) (Optional) Choose where in Drive you want to store the file\n",
        "#    This will be: My Drive / LLM_Math_Cache\n",
        "gdrive_folder = \"/content/gdrive/MyDrive/LLM_Math_Cache\"\n",
        "os.makedirs(gdrive_folder, exist_ok=True)\n",
        "\n",
        "# 3) Upload the JSON from your local machine (Desktop, etc.)\n",
        "print(\"➡️ Choose your 00_clean.json file from your computer in the uploader UI\")\n",
        "uploaded = files.upload()   # This opens a file picker\n",
        "\n",
        "# 4) Move the uploaded file into your Google Drive folder\n",
        "for local_name in uploaded.keys():\n",
        "    src_path = local_name\n",
        "    dst_path = os.path.join(gdrive_folder, local_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "    print(f\"✅ Moved {local_name} → {dst_path}\")\n"
      ],
      "metadata": {
        "id": "djnBtOYQOMy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 7: Verify JSON File uploaded successfully or not."
      ],
      "metadata": {
        "id": "3-i3JKlmOT0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "gdrive_folder = \"/content/gdrive/MyDrive/LLM_Math_Cache\"\n",
        "json_path = Path(gdrive_folder) / \"00_clean.json\"\n",
        "\n",
        "print(f\"Checking for: {json_path}\")\n",
        "\n",
        "# 1) Check existence\n",
        "if json_path.exists():\n",
        "    print(\"✅ File exists in Google Drive!\")\n",
        "\n",
        "    # 2) Optional: list the folder contents\n",
        "    print(\"\\nFiles in folder:\")\n",
        "    !ls -l \"/content/gdrive/MyDrive/LLM_Math_Cache\"\n",
        "\n",
        "    # 3) Optional: try loading it to be 100% sure it's valid JSON\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"\\n✅ Successfully loaded JSON with {len(data)} top-level items.\")\n",
        "else:\n",
        "    print(\"❌ File NOT found. Double-check the folder name and that you uploaded to this location.\")"
      ],
      "metadata": {
        "id": "rZXQj61EOdjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 8: Final Inference Code\n",
        "\n",
        "• This is the final working inference pipeline, used to run mathematical questions through the selected LLM (currently DeepSeek-Math-7B) using 4-bit quantization for memory-efficient execution on limited GPU resources.\n",
        "\n",
        "• It loads a cleaned JSON dataset of math questions, performs deterministic step-by-step inference, and records model responses along with performance metrics such as response time and tokens per second.\n",
        "\n",
        "• The code includes aggressive GPU and memory cleanup, ensuring stable long-running execution and preventing out-of-memory errors during batch inference.\n",
        "\n",
        "• All results are saved to CSV files, including intermediate checkpoints and a final consolidated output, providing the definitive output for evaluation and analysis in this project."
      ],
      "metadata": {
        "id": "1I5Ln6cOOjnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== FINAL WORKING INFERENCE CELL (Default HF Cache) ====================\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import gc\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ------------------- CONFIG -------------------\n",
        "JSON_PATH = Path(\"/content/gdrive/MyDrive/LLM_Math_Cache/00_clean.json\")\n",
        "OUTPUT_CSV = Path(\"00_clean_results_final.csv\")\n",
        "SYSTEM_PROMPT = \"You are a precise, concise, and truthful math assistant. Solve step-by-step.\"\n",
        "\n",
        "MAX_NEW_TOKENS = 1500\n",
        "\n",
        "# 4-bit quantization (fits 7B models easily on free Colab T4)\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# ------------------- MODELS TO TEST -------------------\n",
        "MODELS = [\n",
        "    {\"name\": \"Deepseek-Math-7B\", \"repo\": \"deepseek-ai/deepseek-math-7b-instruct\"},\n",
        "    # {\"name\": \"Qwen2.5-Math-7B-Instruct\", \"repo\": \"Qwen/Qwen2.5-Math-7B-Instruct\"},\n",
        "    # {\"name\": \"Phi-3.5-mini\", \"repo\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
        "]\n",
        "\n",
        "# ------------------- HELPERS -------------------\n",
        "def aggressive_cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "def load_questions(path):\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Loaded {len(data)} questions\")\n",
        "    return data\n",
        "\n",
        "questions = load_questions(JSON_PATH)\n",
        "\n",
        "# ------------------- MAIN LOOP -------------------\n",
        "all_results = []\n",
        "\n",
        "for model_info in MODELS:\n",
        "    name = model_info[\"name\"]\n",
        "    repo = model_info[\"repo\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"LOADING & RUNNING: {name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    aggressive_cleanup()\n",
        "    time.sleep(2)\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading {repo} (4-bit) ...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(repo, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            repo,\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=0.0,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        print(f\"Model {name} loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # ------------------- Inference -------------------\n",
        "    for idx, item in enumerate(tqdm(questions, desc=f\"{name[:20]:<20}\")):\n",
        "        qid = item.get(\"id\", idx)\n",
        "        question = item[\"question\"]\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": question}\n",
        "        ]\n",
        "\n",
        "        start = time.time()\n",
        "        try:\n",
        "            out = pipe(messages)\n",
        "            response = out[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start\n",
        "            out_tokens = len(tokenizer.encode(response))\n",
        "\n",
        "            all_results.append({\n",
        "                \"id\": qid,\n",
        "                \"model\": name,\n",
        "                \"question\": question,\n",
        "                \"response\": response,\n",
        "                \"response_time_sec\": round(elapsed, 3),\n",
        "                \"tokens_output\": out_tokens,\n",
        "                \"tokens_per_sec\": round(out_tokens / max(elapsed, 0.01), 2),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            all_results.append({\n",
        "                \"id\": qid, \"model\": name, \"question\": question,\n",
        "                \"response\": f\"ERROR: {e}\", \"response_time_sec\": -1,\n",
        "                \"tokens_output\": 0, \"tokens_per_sec\": 0,\n",
        "            })\n",
        "\n",
        "        # Light cleanup every 10 questions\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # ------------------- Cleanup model -------------------\n",
        "    del model, tokenizer, pipe\n",
        "    aggressive_cleanup()\n",
        "\n",
        "    # Save checkpoint after each model\n",
        "    pd.DataFrame(all_results).to_csv(f\"results_up_to_{name}.csv\", index=False)\n",
        "    print(f\"Checkpoint saved for {name}\")\n",
        "\n",
        "# ------------------- FINAL SAVE & SUMMARY -------------------\n",
        "df = pd.DataFrame(all_results)\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\nALL DONE! Saved {len(df)} rows → {OUTPUT_CSV}\")\n",
        "\n",
        "if len(df) > 0 and \"response_time_sec\" in df.columns:\n",
        "    valid = df[df[\"response_time_sec\"] > 0]\n",
        "    if len(valid) > 0:\n",
        "        summary = valid.groupby(\"model\").agg({\n",
        "            \"tokens_per_sec\": \"mean\",\n",
        "            \"response_time_sec\": \"mean\",\n",
        "            \"id\": \"count\"\n",
        "        }).round(2).rename(columns={\"id\": \"completed\"})\n",
        "        print(\"\\nPerformance Summary:\")\n",
        "        print(summary.sort_values(\"tokens_per_sec\", ascending=False))"
      ],
      "metadata": {
        "id": "TtwwTIJMOxZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Socratic Debugging Code"
      ],
      "metadata": {
        "id": "WgdWZIp4Sibk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 1: Setup: Drive, GPU, Packages and check the Import and GPU."
      ],
      "metadata": {
        "id": "BE8sWWOaSodj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cache folder\n",
        "!mkdir -p /content/gdrive/MyDrive/LLM_Math_Cache\n",
        "\n",
        "# Upgrade critical packages\n",
        "!pip install -q --upgrade bitsandbytes accelerate transformers\n",
        "!pip install -q torch sentencepiece tqdm pandas huggingface_hub\n",
        "\n",
        "import os, re, time, json, random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import torch\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NO GPU!\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Drive mounted + packages updated\")\n",
        "\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "1G-_l6-2S3IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 2: Login to HuggingFace"
      ],
      "metadata": {
        "id": "nG_y0A5pTE3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# REPLACE WITH YOUR REAL TOKEN → https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"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\"   # ← CHANGE THIS!\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to Hugging Face\")"
      ],
      "metadata": {
        "id": "trAxhYF1Tb5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 3: Load Path, Model, and quantization information"
      ],
      "metadata": {
        "id": "XQxVsp7_TdIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_DIR = \"/content/drive/MyDrive/LLM_Math_Cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/Socratic_Debugging_Raw\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "MODELS = [\n",
        "    {\"name\": \"Deepseek-Math-7B\", \"repo\": \"deepseek-ai/DeepSeek-Math-7B-Instruct\"},\n",
        "    # {\"name\": \"Qwen2.5-Math-7B-Instruct\", \"repo\": \"Qwen/Qwen2.5-Math-7B-Instruct\"},\n",
        "    # {\"name\": \"Phi-3.5-mini\", \"repo\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
        "]\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "aTYOYA6GTrxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 4: Load questions from a JSON file and the Clean and normalize the content."
      ],
      "metadata": {
        "id": "VHooO4rpUK9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTIONS_PATH = \"/content/00_clean.json\"  # change if needed\n",
        "\n",
        "with open(QUESTIONS_PATH, \"r\") as f:\n",
        "    QUESTIONS = json.load(f)\n",
        "\n",
        "print(\"Loaded questions:\", len(QUESTIONS))\n",
        "print(\"First item:\", QUESTIONS[0])\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return str(s)\n",
        "    # fix common mojibake seen in your sample\n",
        "    s = (s.replace(\"â€™\", \"’\")\n",
        "           .replace(\"â€œ\", \"“\")\n",
        "           .replace(\"â€�\", \"”\")\n",
        "           .replace(\"â€“\", \"–\")\n",
        "           .replace(\"â€”\", \"—\")\n",
        "           .replace(\"â€¦\", \"…\")\n",
        "           .replace(\"Ëš\", \"°\"))\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "for q in QUESTIONS:\n",
        "    q[\"question\"] = clean_text(q[\"question\"])\n",
        "\n",
        "PROBLEMS = []\n",
        "for q in QUESTIONS:\n",
        "    PROBLEMS.append({\n",
        "        \"id\": f\"Q{q['id']}\",          # string id\n",
        "        \"qid\": int(q[\"id\"]),          # numeric id\n",
        "        \"difficulty\": q.get(\"difficulty\", \"Unknown\"),\n",
        "        \"human_type\": q.get(\"human_type\", \"Unknown\"),\n",
        "        \"text\": q[\"question\"],\n",
        "    })\n",
        "\n",
        "print(\"Total problems:\", len(PROBLEMS))\n",
        "print(\"Example:\", PROBLEMS[2])\n",
        "\n"
      ],
      "metadata": {
        "id": "PnxTrSYhUR_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CELL 5 — Socratic Debugging prompts"
      ],
      "metadata": {
        "id": "2phfEK4OUudU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TURN1_TEMPLATE = \"\"\"Solve the following problem step-by-step.\n",
        "Explain each reasoning step clearly, including why you chose specific methods or variables.\n",
        "If applicable, identify and justify any assumptions.\n",
        "\n",
        "Problem: {problem}\n",
        "\"\"\"\n",
        "\n",
        "TURN2_TEMPLATE = \"\"\"You must critique and re-evaluate ONLY your previous solution above.\n",
        "\n",
        "Rules:\n",
        "- Do NOT restate the problem.\n",
        "- Do NOT repeat your full Turn 1 solution.\n",
        "- Then give the corrected solution concisely.\n",
        "- Finish with a single line: FINAL: <final answer only> and if it needed to be corrected or not.\n",
        "\n",
        "Now perform the re-evaluation.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Zo8wf0GlUx-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 6: Load model/tokenizer"
      ],
      "metadata": {
        "id": "bf8Tc-7RU3XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(repo: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(repo, cache_dir=CACHE_DIR, use_fast=True)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        repo,\n",
        "        cache_dir=CACHE_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "    # Some tokenizers don't have pad token set; align to eos to avoid warnings\n",
        "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "DWzfzlJ-VBxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 7: Chat Formatting & Response Generation\n",
        "• Formats user, system, and assistant messages into a chat-style prompt, using the model’s native chat template when available for compatibility and correctness.\n",
        "\n",
        "• Provides a fallback prompt construction method to ensure robustness across models that do not support chat templates.\n",
        "\n",
        "• Generates model responses in inference-only mode, disabling gradients to improve performance and reduce memory usage.\n",
        "\n",
        "• Produces clean final answers by removing the prompt text from the generated output, returning only the assistant’s response."
      ],
      "metadata": {
        "id": "c38cbj77VF7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chat_input(tokenizer, messages):\n",
        "    \"\"\"\n",
        "    messages: [{\"role\": \"user\"/\"assistant\"/\"system\", \"content\": \"...\"}]\n",
        "    Uses apply_chat_template when available.\n",
        "    \"\"\"\n",
        "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    # fallback (rare): simple concatenation\n",
        "    s = \"\"\n",
        "    for m in messages:\n",
        "        s += f\"{m['role'].upper()}: {m['content']}\\n\"\n",
        "    s += \"ASSISTANT:\"\n",
        "    return s\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_response(model, tokenizer, messages, max_new_tokens=512, temperature=0.0, top_p=1.0):\n",
        "    prompt = build_chat_input(tokenizer, messages)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    do_sample = temperature > 0\n",
        "\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    if do_sample:\n",
        "        gen_kwargs.update(dict(temperature=temperature, top_p=top_p))\n",
        "\n",
        "    out = model.generate(**inputs, **gen_kwargs)\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # strip prompt prefix if present\n",
        "    if decoded.startswith(prompt):\n",
        "        return decoded[len(prompt):].strip()\n",
        "    return decoded.strip()\n"
      ],
      "metadata": {
        "id": "Wk3m2vI9VbQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 8: Socratic Evaluation & Experiment Execution\n",
        "\n",
        "• Runs the final experiment loop where each model is evaluated over multiple shuffled runs to ensure robustness and reduce order bias.\n",
        "\n",
        "• Performs two-turn generation per problem:\n",
        "\n",
        "1.   Turn 1: the model generates an initial solution.\n",
        "2.   Turn 2: the model critiques and corrects its own previous answer without restating the problem.\n",
        "\n",
        "\n",
        "• Records detailed metadata for each run, including model details, difficulty level, prompts, responses, decoding parameters, and timestamps.\n",
        "\n",
        "• Saves all raw model outputs into a CSV file, producing the final experimental dataset used for quantitative and qualitative analysis."
      ],
      "metadata": {
        "id": "VsRd9AIyVkHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NEW_TOKENS_T1 = 1024\n",
        "MAX_NEW_TOKENS_T2 = 768\n",
        "TEMPERATURE = 0.0\n",
        "TOP_P = 1.0\n",
        "N_RUNS = 3\n",
        "\n",
        "USE_SUBSET = False\n",
        "SUBSET_N = 10\n",
        "problems_to_run = PROBLEMS[:SUBSET_N] if USE_SUBSET else PROBLEMS\n",
        "\n",
        "SYSTEM_MSG = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"Follow instructions exactly. Do not repeat the problem unless asked. In Turn 2, critique the prior answer rather than restarting.\"\n",
        "}\n",
        "\n",
        "TURN2_TEMPLATE = \"\"\"You must critique and re-evaluate ONLY your previous solution above.\n",
        "\n",
        "Rules:\n",
        "- Do NOT restate the problem.\n",
        "- Do NOT repeat your full Turn 1 solution.\n",
        "- Start by identifying the first incorrect step (if any), or say \"No conceptual error found\".\n",
        "- Provide a corrected solution concisely.\n",
        "- Finish with a single line: FINAL: <final answer only>and if it needed to be corrected or not.\n",
        "\n",
        "Now perform the re-evaluation.\n",
        "\"\"\"\n",
        "\n",
        "RESULTS = []\n",
        "\n",
        "for m in MODELS:\n",
        "    print(f\"\\n=== Loading: {m['name']} ({m['repo']}) ===\")\n",
        "    model, tokenizer = load_model(m[\"repo\"])\n",
        "\n",
        "    for run_id in range(1, N_RUNS + 1):\n",
        "        probs = problems_to_run.copy()\n",
        "        seed = abs(hash((m[\"name\"], run_id))) % (2**32)\n",
        "        rnd = random.Random(seed)\n",
        "        rnd.shuffle(probs)\n",
        "\n",
        "        for p in tqdm(probs, desc=f\"{m['name']} run {run_id}\", leave=False):\n",
        "            q_text = p[\"text\"]\n",
        "\n",
        "            turn1_prompt = TURN1_TEMPLATE.format(problem=q_text)\n",
        "\n",
        "            # Turn 1\n",
        "            messages1 = [SYSTEM_MSG, {\"role\": \"user\", \"content\": turn1_prompt}]\n",
        "            t1 = generate_response(\n",
        "                model, tokenizer, messages1,\n",
        "                max_new_tokens=MAX_NEW_TOKENS_T1,\n",
        "                temperature=TEMPERATURE,\n",
        "                top_p=TOP_P,\n",
        "            )\n",
        "\n",
        "            # Turn 2\n",
        "            turn2_prompt = TURN2_TEMPLATE\n",
        "            messages2 = [\n",
        "                SYSTEM_MSG,\n",
        "                {\"role\": \"user\", \"content\": turn1_prompt},\n",
        "                {\"role\": \"assistant\", \"content\": t1},\n",
        "                {\"role\": \"user\", \"content\": turn2_prompt},\n",
        "            ]\n",
        "            t2 = generate_response(\n",
        "                model, tokenizer, messages2,\n",
        "                max_new_tokens=MAX_NEW_TOKENS_T2,\n",
        "                temperature=TEMPERATURE,\n",
        "                top_p=TOP_P,\n",
        "            )\n",
        "\n",
        "            RESULTS.append({\n",
        "                \"model\": m[\"name\"],\n",
        "                \"repo\": m[\"repo\"],\n",
        "                \"run_id\": run_id,\n",
        "                \"shuffle_seed\": seed,\n",
        "\n",
        "                \"question_id\": p[\"qid\"],\n",
        "                \"problem_id\": p[\"id\"],\n",
        "                \"difficulty\": p[\"difficulty\"],\n",
        "                \"human_type\": p[\"human_type\"],\n",
        "\n",
        "                \"question_text\": q_text,\n",
        "\n",
        "                \"turn1_prompt\": turn1_prompt,\n",
        "                \"turn1_response\": t1,\n",
        "\n",
        "                \"turn2_prompt\": turn2_prompt,\n",
        "                \"turn2_response\": t2,\n",
        "\n",
        "                \"max_new_tokens_t1\": MAX_NEW_TOKENS_T1,\n",
        "                \"max_new_tokens_t2\": MAX_NEW_TOKENS_T2,\n",
        "                \"temperature\": TEMPERATURE,\n",
        "                \"top_p\": TOP_P,\n",
        "\n",
        "                \"timestamp_utc\": int(time.time()),\n",
        "            })\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "df = pd.DataFrame(RESULTS)\n",
        "print(\"Rows:\", len(df))\n",
        "\n",
        "csv_path = os.path.join(OUT_DIR, f\"socratic_debugging_raw_{int(time.time())}.csv\")\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved raw responses to:\", csv_path)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4-PY0jvoWLgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}